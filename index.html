<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd">
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" lang="en"><head>
  <title>Nerf-Neurons Project Page</title>
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">

<meta property="og:image" content="resources/teaser.jpg"/>
<meta property="og:title" content="Searching for Semantic Neurons in NeRF" />

<script src="src/lib.js" type="text/javascript"></script>
<script src="src/popup.js" type="text/javascript"></script>


<script type="text/javascript">
// redefining default features
var _POPUP_FEATURES = 'width=500,height=300,resizable=1,scrollbars=1,titlebar=1,status=1';
</script>

<link media="all" href="src/glab.css" type="text/css" rel="StyleSheet">
<style type="text/css" media="all">
IMG {
	PADDING-RIGHT: 0px;
	PADDING-LEFT: 0px;
	FLOAT: right;
	PADDING-BOTTOM: 0px;
	PADDING-TOP: 0px
}
#primarycontent {
	MARGIN-LEFT: auto; ; WIDTH: expression(document.body.clientWidth >
800? "800px": "auto" ); MARGIN-RIGHT: auto; TEXT-ALIGN: left; max-width:
800px }
BODY {
	TEXT-ALIGN: center
}
</style>

<meta content="MSHTML 6.00.2800.1400" name="GENERATOR"><script src="b5m.js" id="b5mmain" type="text/javascript"></script></head>

<body>

<div id="primarycontent">
<center><h1>Fantastic NeRF Neurons and Where to Find Them</h1></center>
<center><h2>
  <a href="https://gauravparmar.com/">Gaurav Parmar</a>&nbsp;&nbsp;&nbsp;
  <a href="TBD">Bowei Chen</a>&nbsp;&nbsp;&nbsp;
</h2>
</center>
<center><h2>
  Carnegie Mellon University &nbsp;&nbsp;&nbsp;
</h2></center>
<center>
  <h2>
      <strong>
          <a href="TBD">[Report]</a> | 
          <a href="TBD">[GitHub]</a> |
          <a href="TBD">[YouTube]</a> |
          <a href="TBD">[Slides]</a>
      </strong>
  </h2></center>
<center style="margin-top:1cm;">
  <a href="resources/teaser.png">
  <img src="resources/teaser.png" height="100">
  </a>
</center>
<p></p>

<br><br>
<p style="margin-top:5cm;">
<h2>Abstract</h2>
<div style="font-size:14px"><p align="justify">
    [TODO: Summary of all results and conclusions]
</p></div>
<br><br>

<p style="margin-top:1cm;">
<h2 align='center' style="color: #990000;"><strong>Introduction</strong></h2>
<br>
<div style="font-size:14px"><p align="justify">
     Novel view synthesis is a classical problem at the intersection of computer vision and graphics.
    Given a number of images taken from different camera poses, novel view synthetic aims to produce realistic image of the scene from
    novel angles[1]. Recently, NeRF [2] makes significant progress in this area by utilizing neural scene representation. It takes as input
    a 5D camera pose representation and uses scene-specific multi-layer perceptrons (MLPs) to represent the whole scene. Compared to explict representation like
    point clouds and voxels, this representation is compact and able to reconstruct higher resolution images.
    <br>
        <br>

    Although NeRF performs well in many cases, it still has the following limitation: the learnt scene representation is very rigid and cannot be edited
    flexibly. In other words, all the scene details are fixed in the representation and it is hard to edit one small part without messing up the rest part of the scene.
    To illustrate, suppose we have learnt scene representation of the scene with 100 spheres. If we want to remove a sphere from the scene, we have to
    retrain the network instead of editing the learnt representation directly.
    <br>
    <br>

    To edit the scene representation in a more flexible way, we need to gain more understanding of  the learnt scene representation in different levels.
    Inspired by the recent paper [3] that tries to visualize and understand GAN at the unit-, object-, and scene-level, we present a simple framework to
    study and edit NeRF representation. In specific, we try to answer the following questions: Are there some neurons responsible for controlling
    the emergence of an object in the scene? Can we edit the scene at instance level given a specific operation?
    <br>
    <br>

    In summary, the contributions of our project can be summarized as follows:
    (1) We present a simple framework to study and edit NeRF representation.

    (2) We render a small dataset and conduct some experiments to show that our proposed framework can effectively study and manipulate the NeRF
    representation.


</p></div>

<br><br>
<p style="margin-top:1cm;">
<h2 align='center' style="color: #990000;"><strong>Related Works</strong></h2>

<h3>Novel View Synthesis</h3>
<!--[TODO: recent nerf explosion and short description of some followup works]-->
The task of novel view synthesis is to reconstruct an unseen view given a number of input views. [4] propose a light-field based method which can produce
    realistic results but required a dense set of images. Recent methods [2][5][6] reconstruct novel views with a sparse
    set of input images by using deep neural network. Among these methods, NeRF [2] is the most popular one in the area.
    NeRF adopts an MLP to learn a 5D radiance field of the scene by directly regresses the volume density and RGB colors.
    Since the camera pose is a part of input of the method, the output of NeRF can leverage the view-dependent effect of
    the scene to some extent.

<h3>Interpretable Networks</h3>
[TODO: Ganseeing]


<br><br>
<p style="margin-top:1cm;">
<h2 align='center' style="color: #990000;"><strong>Preliminaries</strong></h2>

<h3>Vanilla NeRF:</h3>
[TODO: describe the nerf formution precisely and mathematically]

<h3>Latent Space and Overfitting:</h3>
[TODO: no natural latent space] <br>
[TODO: no generalization across scenes]


<br><br>
<p style="margin-top:1cm;">
<h2 align='center' style="color: #990000;"><strong>Method</strong></h2>
In this section we describe the method we use to find semantically meaningful neurons under different settings.
First, we we describe XXX <br>
Next, we describe YYY <br>
Finally, we show ABC in figs 123 <br>
<h3>Dataset Used: </h3>
[TODO: describe the image used, how it is generated and why]
<h3>Baseline Model: </h3>
[TODO: nerf with the architecuture and training details]
<h3>Modified Training Procedure: </h3>
[TODO: our modifications applied]
<h3>Evaluation</h3>
[TODO: the task is to remove a sphere from the scene] <br>
[TODO: yikes]


<br><br>
<p style="margin-top:1cm;">
<h2 align='center' style="color: #990000;"><strong>Experiments</strong></h2>
<br>
<h3 style="color: #666666;">Grid Search over Neurons:</h3>
We begin our experiments by investigating a traditional NeRF model that is trained on 
the scene shown in Figure [TODO]. We iterate through all neurons in the trained model 
one at a time and perturb its output by various scalar factors (1e-2, 1e-1, 10, 100, 100). 
For each neuron and the corresponding perturbation applied, we observe render the scene 
from a fixed camera pose and inspect the change induced by the perturbation. 
<br>
Using this procedure above we search for a neuron that corresponds to the task of 
removing a specific sphere from the scene. The 18th neuron in the first layer of network 
is found to best correspond to this edit. In Figure XX above we show the edited scene generated 
by perturbing this neuron. We observe that even the best perturbation does not capture the 
desired edit in the scene and introduces numerous artifacts in the other regions of the image. 
<br> <br>
[<strong>FIGURE TODO ::: </strong> NeRF neuron-18]<br>
[<strong>FIGURE TODO ::: </strong> other neurons found]
<br><br>

<h3 style="color: #666666;">Transformation Conditioned NeRF Training: </h3>
We hypothesize that the training objective of the NeRF does not encourage the 
learning of semantically neuron representations that are disentangled and control the attributes 
in a consistent manner. In order to test this, we modify the NeRF training procedure as shown 
in the Figure YY below. 
<br> <br>
[<strong>FIGURE TODO ::: </strong> modified NeRF training]
<br><br>
The Figure YY below shows that the modified NeRF training is capable of learning a more 
semantic representation of the scene. The modified scene captures the desired edit 
consistently across different camera poses and does not introduce severe artifects in 
other parts of the scene. 

<h3 style="color: #666666;">Quantiative Comparision: </h3>

<h3 style="color: #666666;">Compositionality of Edits: </h3>

<!-- 
<table border="0" cellspacing="0" cellpadding="0">
  <tr>
    <tr>
        <td align="center" valign="middle"><a href=".resources/dcvae_results.png"><img src="resources/dcvae_results.png"  width=800> </a></td>
    </tr>
  </tr>

</table> -->
<!-- <p>&nbsp;</p> -->

<br>
<h2 align='center' style="color: #990000;"><strong>Acknowledgements</strong></h2>
<p align="justify">
  [TODO: ackowledge course staff and other relevant people]
</p>

<br>
<h2>Citation</h2>
<p><a href="https://cseweb.ucsd.edu/~ravir/6998/papers/p279-chen.pdf">[1] Shenchang Eric Chen and Lance Williams. View interpolationfor image synthesis.SIGGRAPH, 1993 </a>. </p>
<p><a href="https://arxiv.org/pdf/2102.13090.pdf">[2] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng.  Nerf: Representing scenes as neural radiance fields for view synthesis.ECCV, 2020. </a> </p>
<p><a href="https://arxiv.org/pdf/1811.10597.pdf">[3] David Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua B. Tenenbaum, William T. Freeman, Antonio Torralba.  Nerf: Representing scenes as neural radiance fields for view synthesis.ECCV, 2020. </a> </p>
<p><a href="https://arxiv.org/pdf/1811.10597.pdf">[4] Marc  Levoy  and  Pat  Hanrahan.   Light  field  rendering.  SIGGRAPH 1996. </a>. </p>
<p><a href="https://arxiv.org/pdf/1904.04290.pdf">[5] Moustafa Meshry, Dan B Goldman, Sameh Khamis, HuguesHoppe,  Rohit Pandey,  Noah Snavely, and Ricardo Martin-Brualla. Neural rerendering in the wild. CVPR, 2019. </a> </p>
<p><a href="https://research.fb.com/publications/neural-volumes-learning-dynamic-renderable-volumes-from-images/">[6] Stephen  Lombardi,  Tomas  Simon,  Jason  Saragih,  GabrielSchwartz, Andreas Lehrmann, and Yaser Sheikh. Neural vol-umes:  Learning dynamic renderable volumes from images. ACM Trans. Graph. 2019. </a> </p>





</div>
</body></html>