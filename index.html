<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd">
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" lang="en"><head>
  <title>Nerf-Neurons Project Page</title>
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">

<meta property="og:image" content="resources/teaser.jpg"/>
<meta property="og:title" content="Searching for Semantic Neurons in NeRF" />

<script src="src/lib.js" type="text/javascript"></script>
<script src="src/popup.js" type="text/javascript"></script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>

<script type="text/javascript">
// redefining default features
var _POPUP_FEATURES = 'width=500,height=300,resizable=1,scrollbars=1,titlebar=1,status=1';
</script>

<link media="all" href="src/glab.css" type="text/css" rel="StyleSheet">
<style type="text/css" media="all">
IMG {
	PADDING-RIGHT: 0px;
	PADDING-LEFT: 0px;
	FLOAT: right;
	PADDING-BOTTOM: 0px;
	PADDING-TOP: 0px
}
#primarycontent {
	MARGIN-LEFT: auto; ; WIDTH: expression(document.body.clientWidth >
800? "800px": "auto" ); MARGIN-RIGHT: auto; TEXT-ALIGN: left; max-width:
800px }
BODY {
	TEXT-ALIGN: center
}
</style>

<meta content="MSHTML 6.00.2800.1400" name="GENERATOR"><script src="b5m.js" id="b5mmain" type="text/javascript"></script></head>

<body>

<div id="primarycontent">
<center><h1>Fantastic NeRF Neurons and Where to Find Them</h1></center>
<center><h2>
  <a href="https://gauravparmar.com/">Gaurav Parmar</a>&nbsp;&nbsp;&nbsp;
  <a href="https://armastuschen.github.io/">Bowei Chen</a>&nbsp;&nbsp;&nbsp;
</h2>
</center>
<center><h2>
  Carnegie Mellon University &nbsp;&nbsp;&nbsp;
</h2></center>
<center>
  <h2>
      <strong>
          <a href="TBD">[Report]</a> | 
          <a href="TBD">[GitHub]</a> |
          <a href="TBD">[YouTube]</a> |
          <a href="TBD">[Slides]</a>
      </strong>
  </h2></center>
<center style="margin-top:1cm;">
  <a href="resources/teaser.png">
  <img src="resources/teaser.png" height="100">
  </a>
</center>
<p></p>

<br><br>
<p style="margin-top:5cm;">
<h2 style="color: #990000;"> <strong>Abstract </strong></h2>
<div style="font-size:14px"><p align="justify">
    Neural Radiance Fields (NeRF) achieve impressive results in novel view synthesis. However, it still suffers from the following problem:
    the learnt scene representation is fixed and not allowed to be edited flexibly. In this project, we will study and understand the scene representation
    at different levels, which will allow us to perform user-defined operations on the scene by editing its corresponding representation.
    We use two approaches to investigate and visualize
    the edited representation, and conduct experiments to show that our approaches can effectively study and manipulate the NeRF representation.
</p></div>
<br><br>

<p style="margin-top:1cm;">
<h2 align='center' style="color: #990000;"><strong>Introduction</strong></h2>
<br>
<div style="font-size:14px"><p align="justify">
     Novel view synthesis is a classical problem at the intersection of computer vision and graphics.
    Given a number of images taken from different camera poses, novel view synthetic aims to produce realistic image of the scene from
    novel angles[1]. Recently, NeRF [2] makes significant progress in this area by utilizing neural scene representation. It takes as input
    a 5D camera pose representation and uses scene-specific multi-layer perceptrons (MLPs) to represent the whole scene. Compared to explict representation like
    point clouds and voxels, this representation is compact and able to reconstruct higher resolution images.
    <br>
        <br>

    Although NeRF performs well in many cases, it still has the following limitation: the learnt scene representation is very rigid and cannot be edited
    flexibly. In other words, all the scene details are fixed in the representation and it is hard to edit one small part without messing up the rest part of the scene.
    To illustrate, suppose we have learnt scene representation of the scene with 100 spheres. If we want to remove a sphere from the scene, we have to
    retrain the network instead of editing the learnt representation directly.
    <br>
    <br>

    To edit the scene representation in a more flexible way, we need to gain more understanding of  the learnt scene representation in different levels.
    Inspired by the recent paper [3] that tries to visualize and understand GAN at the unit-, object-, and scene-level, we present a simple framework to
    study and edit NeRF representation. In specific, we try to answer the following questions: Are there some neurons responsible for controlling
    the emergence of an object in the scene? Can we edit the scene at instance level given a specific operation?
    <br>
    <br>

    In summary, the contributions of our project can be summarized as follows:
    (1) We present a simple framework to study and edit NeRF representation.

    (2) We render a small dataset and conduct some experiments to show that our proposed framework can effectively study and manipulate the NeRF
    representation.


</p></div>

<br><br>
<p style="margin-top:1cm;">
<h2 align='center' style="color: #990000;"><strong>Related Works</strong></h2>

<h3 style="color: #666666;">Novel View Synthesis</h3>
The task of novel view synthesis is to reconstruct an unseen view given a number of input views. [4] propose a light-field based method which can produce
    realistic results but required a dense set of images. Recent methods [2][5][6] reconstruct novel views with a sparse
    set of input images by using deep neural network. Among these methods, NeRF [2] is the most popular one in the area.
    NeRF adopts an MLP to learn a 5D radiance field of the scene by directly regresses the volume density and RGB colors.
    Since the camera pose is a part of input of the method, the output of NeRF can leverage the view-dependent effect of
    the scene to some extent. However, NeRF still suffers from the following limitations: (1) The inference time is high.
    (2) The learnt representation is scene specific and it is hard to edit it accordingly.
    <br>
    <br>
    To address the first issue, NSVF [7] reduce the inference time of NeRF by defining a set of voxel-bounded implicit
    fields represented by sparse voxel octree to model local features in each cell. FastNeRF [8] presents
    a NeRF-based system capable  of  rendering photorealistic  novel  views thousands of times faster than original NeRF.

    For the second problem, GIRAFFE [9] combine a compositional 3D scene representation into the generative model to allow more controllable image synthesis.
    Representing  scenes  as  compositional  generative  neural  feature  fields  allows  them to  disentangle
    objects from the background so as to manipulate the scene. [10] propose to learn object-centric neural scattering functions (OSFs)
    that can implicitly model object-level light transport using a neural network.
    It enables rendering scenes even when objects or lights move without retraining.
    <br>
    <br>
    The difference between our project and these methods are that we are focusing on study and understand NeRF, and then edit the scene
    according to the insight we learn from the NeRF representation. We believe this can lead to a more general way to manipulate the scene
    based on NeRF.



<!--<h3>Interpretable Networks</h3>-->
<!--[TODO: Ganseeing]-->


<br><br>
<p style="margin-top:1cm;">
<h2 align='center' style="color: #990000;"><strong>Preliminaries</strong></h2>

<h3 style="color: #666666;">Vanilla NeRF:</h3>
A scene can be modeled as a 5D vector-valued function \(F_{\Theta} \) who takes a 3D camera position  \( \mathbf{x} = (x,y,z)  \) and
    a 2D viewing direction \( \mathbf{d} (\theta, \phi) \) as input, an color value \( \mathbf{c} = (r,g,b) \) and
    volume density \( \sigma \) as output.

Then we can render the color value of each pixel in our target image using principles from classical volume rendering.
In specific, the volume density \( \sigma(\mathbf{x}) \) can be regarded as the probability that the ray
    \(\mathbf{r}(t) = o + t \mathbf{d} \) will terminate at an infinitesimal particle  at location \( \mathbf{x} \), where \(t \) is
    a depth constant. Now the color value of each ray (pixel) can be represented as:

    $$
    C(\mathbf{r}) = \int_{t_n}^{t_f} T(t) \sigma(\mathbf{r}(t) \mathbf{c}(\mathbf{r}(t), \mathbf{d})) dt,
     $$
    where \( T(t) =  - \int_{t_n}^{t} \sigma(\mathbf{r}(s))ds \), specifying the  probability of termination
    of the ray travel from \( t_n \) to \( t \).

    We show how to compute the color value of a ray in a continuous way. In practice, we cannot achieve
    this due to the limitation of computational resource. Therefore, we can divide \( [t_n, t_f] \) into
    \( N \) evenly-spaced  bins and draw one sample from the bins uniformly to get a set of \(t \).
     we can write the discrete version of \(C(\mathbf{r}) \) with the quadrature rule:

    $$
    C(\mathbf{r}) = \sum_{i=1}^N T_i (1 - exp(\sigma_i \Delta_i)) \mathbf{c_i},     $$
    where \(T_i = exp( - \sum_{j=1}^{i-1} \sigma_j \Delta_i) \)
    and \(\Delta_i = t_{i+1} - t_i \) denote the distance betweem two sample positions.
    Note that this calculation rewrite the traditional alpha value as
    \( \alpha_i = 1 - exp(-\sigma_i \Delta_i) \).
    Now we can compute the \( L2 \) loss between the groud truth and output color value
    for the optimization of network parameter \( \Theta \). For more details, please refer to
    the original NeRF paper [2].



<br><br>
<p style="margin-top:1cm;">
<h2 align='center' style="color: #990000;"><strong>Method</strong></h2>
<br>
<h3 style="color: #666666;">Dataset Used: </h3>
<div style="font-size:14px;margin-top:0.25cm;"><p align="justify"></p>
We create a basic scene containing 15 spheres using Mitsuba. To find semantically meaningful neurons for scene representation,
we create 15 additional scenes, and each scene is same as the basic scene excepting for the removal of one specific sphere. We

set the near (minimal scene depth for display) and far (maximum scene depth for display) bound to be 0 and 5 respectively. 
In the figure below we show the initial scene containing all the 15 
spheres on the left. In the right, we show one of the additional scene generated where one of the spheres is removed.
</div>
<table border="0" cellspacing="0" cellpadding="0" style="margin-top:0cm;">
  <tr>
    <tr>
        <td align="center" valign="middle">
          <a href=".resources/ds_base.gif">
            <img src="resources/ds_base.gif"  align="center" width=350>
          </a>
        </td>
        <td align="center" valign="middle" style="padding-left:100px;">
          <a href="resources/ds_edit_5.gif">
            <img src="resources/ds_edit_5.gif"  align="center" width=350>
          </a>
        </td>
    </tr>
  </tr>
</table>

<h3 style="color: #666666;">Baseline Model: </h3>
<div style="font-size:14px;margin-top:0.25cm;"><p align="justify"></p>
We use a PyTorch implementation of NeRF provided <a href="https://github.com/yenchenlin/nerf-pytorch">here</a> for our experiments. 
We do not modify the training hyperparameters and use the default values provided by the authors. 
A configuration file that contains all the hyperparameters used is attached <a href="TBD">here</a>. 
</div>

<h3 style="color: #666666;">Modified Training Procedure: </h3>
<div style="font-size:14px;margin-top:0.25cm;"><p align="justify"></p>
In addition to the original NeRF method we test the effects of modifying the NeRF training objective. 
Instead of training on a single scene, we apply a transformation to the current image and concatenate a 
one-hot embedding of the transformation to the input of the model. These changes made to the training 
process are shown in the Figure below. 
</div>

<table border="0" cellspacing="0" cellpadding="0" style="margin-top:1cm;">
  <tr>
    <tr>
        <td align="center" valign="middle">
          <a href=".resources/architecture.png">
            <img src="resources/architecture.png"  align="center" width=800>
          </a>
        </td>
    </tr>
  </tr>
</table>

<br><br>
<p style="margin-top:1cm;">
<h2 align='center' style="color: #990000;"><strong>Experiments</strong></h2>
<br>
<h3 style="color: #666666;">Search over NeRF Neurons:</h3>
<div style="font-size:14px;margin-top:0.25cm;"><p align="justify">
  We begin our experiments by investigating the vanilla NeRF model that is trained on 
  the full scene (containing all 15 spheres). We iterate through all neurons in the trained model 
  one at a time and perturb its output by various scalar factors (1e-2, 1e-1, 10, 100, 100). 
  For each neuron and the perturbation applied, we render the scene using a fixed camera pose 
  and inspect the change that is induced in the generated scene by the perturbation. 
  <br>
  Using this procedure above we attempt to find individual neurons that can control meaningful 
  attributes of the scene represented. In the figures below we show few interesting neurons that 
  correspond the "middle row of spheres", "middle column of spheres", and "darkness of all spheres". 
  <br>
</p></div>

<table border="0" cellspacing="0" cellpadding="0" style="margin-top:0.5cm;margin-bottom:0.5cm;">
  <tr>
    <tr>
        <td align="center" valign="middle">
          <a href=".resources/interesting_neurons.png">
            <img src="resources/interesting_neurons.png"  align="center" width=800>
          </a>
        </td>
    </tr>
  </tr>
</table>
<div style="font-size:14px"><p align="justify"></p>
Following this observation we attempt to use the same procedure to find individual neuron that 
is capable of editing the scene in a more fine-grained manner. Specifically, we test if this 
method can be used to remove an individual user-specified sphere from the scene. The figure below 
shows the edited scene generated by perturbing the neuron found to be closest to the task. 
We observe that even the best perturbation does not capture the 
desired edit in the scene and introduces numerous artifacts in the other regions of the image. 
</p></div>

<table border="0" cellspacing="0" cellpadding="0" style="margin-top:0.5cm;margin-bottom:0.5cm;">
  <tr>
    <tr>
        <td align="center" valign="middle">
          <a href=".resources/neuron_18.png">
            <img src="resources/neuron_18.png"  align="center" width=800>
          </a>
        </td>
    </tr>
  </tr>
</table>

<br>
<h3 style="color: #666666;">Edits with Transformation Conditioned NeRF: </h3>
<div style="font-size:14px;margin-top:0.25cm;"><p align="justify"></p>
We hypothesize that the original training objective of the NeRF does not encourage the 
learning of semantically neuron representations that are disentangled and control the attributes 
in a consistent manner. In order to test this, we modify the NeRF training procedure as described 
in the Section above and perform the same task of removing an individual sphere from the scene. 
The result in Figure below shows that the modified NeRF training is capable of learning a more 
semantic representation of the scene. The modified scene captures the desired edit 
consistently across different camera poses and does not introduce severe artifects in 
other parts of the scene. 
</div>
<table border="0" cellspacing="0" cellpadding="0" style="margin-top:0.5cm;margin-bottom:0.5cm;">
  <tr>
    <tr>
        <td align="center" valign="middle">
          <a href=".resources/edit_ours.png">
            <img src="resources/edit_ours.png"  align="center" width=800>
          </a>
        </td>
    </tr>
  </tr>
</table>


<h3 style="color: #666666;">Quantiative Comparision: </h3>
<div style="font-size:14px;margin-top:0.25cm;"><p align="justify"></p>
In the <a href="">Figure</a> and <a href="">Figure</a> above we visually showed the results 
of editing the scene. In the table below we confirm our observations by computing the PSNR 
(higher is better) and MSE (lower is better) values between the target scene and edited scene 
generated by the two methods.
</div>
<style type="text/css">
  .tg  {border-collapse:collapse;border-spacing:0;}
  .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
    overflow:hidden;padding:10px 20px;word-break:normal;}
  .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
    font-weight:normal;overflow:hidden;padding:10px 20px;word-break:normal;}
  .tg .tg-9wq8{border-color:inherit;text-align:center;vertical-align:middle}
  .tg .tg-xbtx{font-family:"Times New Roman", Times, serif !important;;text-align:center;vertical-align:middle}
  .tg .tg-nrix{text-align:center;vertical-align:middle}
  </style>
  <table class="tg" style="margin-left: auto; margin-right: auto;margin-top:0.5cm">
  <thead>
    <tr>
      <th class="tg-9wq8"><h3 style="color: #666666;">Model Name</h3></th>
      <th class="tg-xbtx"><h3 style="color: #666666;">Pose Used</h3></th>
      <th class="tg-9wq8"><h3 style="color: #666666;">PSNR</h3></th>
      <th class="tg-9wq8"><h3 style="color: #666666;">MSE</h3></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td class="tg-9wq8" rowspan="2">Traditional NeRF</td>
      <td class="tg-nrix"><img src="resources/gt_1.png"  align="center" width=100></td>
      <td class="tg-9wq8">24.998</td>
      <td class="tg-9wq8">205.726</td>
    </tr>
    <tr>
      <td class="tg-nrix"><img src="resources/gt_2.png"  align="center" width=100></td>
      <td class="tg-nrix">24.461</td>
      <td class="tg-nrix">232.788</td>
    </tr>
    <tr>
      <td class="tg-9wq8" rowspan="2">Transformation Conditioned NeRF</td>
      <td class="tg-nrix"><img src="resources/gt_1.png"  align="center" width=100></td>
      <td class="tg-9wq8">26.015</td>
      <td class="tg-9wq8">162.787</td>
    </tr>
    <tr>
      <td class="tg-nrix"><img src="resources/gt_2.png"  align="center" width=100></td>
      <td class="tg-nrix">26.292</td>
      <td class="tg-nrix">152.714</td>
    </tr>
  </tbody>
  </table>

<br>
<h3 style="color: #666666;">Compositionality of Edits: </h3>
<div style="font-size:14px;margin-top:0.25cm;"><p align="justify"></p>
The figure below shows that the edits performed using our method can be composed together 
directly to generate scenes that the NeRF has never seen during the training. This further 
shows that the scene representation and the edits learnt by our method are not overfit to 
a specific task and encode meaningful details about the scene. 
</div>

<table border="0" cellspacing="0" cellpadding="0" style="margin-top:0.5cm;margin-bottom:0.5cm;">
  <tr>
    <tr>
        <td align="center" valign="middle">
          <a href=".resources/composing_edits.png">
            <img src="resources/composing_edits.png"  align="center" width=800>
          </a>
        </td>
    </tr>
  </tr>
</table>

<br>
<h2 align='center' style="color: #990000;"><strong>Acknowledgements</strong></h2>
<div style="font-size:14px;margin-top:0.25cm;"><p align="justify"></p>
We thank the instructors for useful discussions and suggestions about the project. 
</div>

<br>
<h2 style="color: #990000;"><strong>Citations</strong></h2>
<br>
<p><a href="https://cseweb.ucsd.edu/~ravir/6998/papers/p279-chen.pdf">[1]</a> Shenchang Eric Chen and Lance Williams. View interpolationfor image synthesis.SIGGRAPH, 1993. </p>
<p><a href="https://arxiv.org/pdf/2102.13090.pdf">[2]</a> Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng.  Nerf: Representing scenes as neural radiance fields for view synthesis.ECCV, 2020. </p>
<p><a href="https://arxiv.org/pdf/1811.10597.pdf">[3]</a>  David Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua B. Tenenbaum, William T. Freeman, Antonio Torralba.  Nerf: Representing scenes as neural radiance fields for view synthesis.ECCV, 2020. </p>
<p><a href="https://arxiv.org/pdf/1811.10597.pdf">[4]</a>  Marc  Levoy  and  Pat  Hanrahan.   Light  field  rendering.  SIGGRAPH 1996. </p>
<p><a href="https://arxiv.org/pdf/1904.04290.pdf">[5]</a> Moustafa Meshry, Dan B Goldman, Sameh Khamis, HuguesHoppe,  Rohit Pandey,  Noah Snavely, and Ricardo Martin-Brualla. Neural rerendering in the wild. CVPR, 2019. </p>
<p><a href="https://research.fb.com/publications/neural-volumes-learning-dynamic-renderable-volumes-from-images/">[6]</a> Stephen  Lombardi,  Tomas  Simon,  Jason  Saragih,  GabrielSchwartz, Andreas Lehrmann, and Yaser Sheikh. Neural vol-umes:  Learning dynamic renderable volumes from images. ACM Trans. Graph. 2019. </p>
<p><a href="https://arxiv.org/abs/2007.11571">[7]</a>   Lingjie  Liu,  Jiatao  Gu,  Kyaw  Zaw  Lin,  Tat-Seng  Chua,and Christian Theobalt.   Neural sparse voxel fields.arXivpreprint arXiv:2007.11571, 2020. </p>
<!--<p><a href="https://arxiv.org/abs/2010.07492">[8]   Kai  Zhang,  Gernot  Riegler,  Noah  Snavely,  and  VladlenKoltun.  Nerf++:  Analyzing and improving neural radiancefields.   arXiv preprint arXiv:2010.07492, 2020 </a> </p>-->
<p><a href="https://arxiv.org/abs/2103.10380">[8]</a> Stephan J. Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, Julien Valentin.  FastNeRF: High-Fidelity Neural Rendering at 200FPS. CVPR, 2021. </p>
<p><a href="https://arxiv.org/abs/2011.12100">[9]</a> Michael Niemeyer, Andreas Geiger. GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields. CVPR, 2021. </p>
<p><a href="https://arxiv.org/abs/2012.08503">[10]</a> Michelle Guo, Alireza Fathi, Jiajun Wu, Thomas Funkhouser. Object-Centric Neural Scene Rendering. CVPR, 2020. </p>





</div>
</body></html>