<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd">
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" lang="en"><head>
  <title>Nerf-Neurons Project Page</title>
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">

<meta property="og:image" content="resources/teaser.jpg"/>
<meta property="og:title" content="Searching for Semantic Neurons in NeRF" />

<script src="src/lib.js" type="text/javascript"></script>
<script src="src/popup.js" type="text/javascript"></script>


<script type="text/javascript">
// redefining default features
var _POPUP_FEATURES = 'width=500,height=300,resizable=1,scrollbars=1,titlebar=1,status=1';
</script>

<link media="all" href="src/glab.css" type="text/css" rel="StyleSheet">
<style type="text/css" media="all">
IMG {
	PADDING-RIGHT: 0px;
	PADDING-LEFT: 0px;
	FLOAT: right;
	PADDING-BOTTOM: 0px;
	PADDING-TOP: 0px
}
#primarycontent {
	MARGIN-LEFT: auto; ; WIDTH: expression(document.body.clientWidth >
800? "800px": "auto" ); MARGIN-RIGHT: auto; TEXT-ALIGN: left; max-width:
800px }
BODY {
	TEXT-ALIGN: center
}
</style>

<meta content="MSHTML 6.00.2800.1400" name="GENERATOR"><script src="b5m.js" id="b5mmain" type="text/javascript"></script></head>

<body>

<div id="primarycontent">
<center><h1>Fantastic NeRF Neurons and Where to Find Them</h1></center>
<center><h2>
  <a href="https://gauravparmar.com/">Gaurav Parmar</a>&nbsp;&nbsp;&nbsp;
  <a href="https://armastuschen.github.io/">Bowei Chen</a>&nbsp;&nbsp;&nbsp;
</h2>
</center>
<center><h2>
  Carnegie Mellon University &nbsp;&nbsp;&nbsp;
</h2></center>
<center>
  <h2>
      <strong>
          <a href="TBD">[Report]</a> | 
          <a href="TBD">[GitHub]</a> |
          <a href="TBD">[YouTube]</a> |
          <a href="TBD">[Slides]</a>
      </strong>
  </h2></center>
<center style="margin-top:1cm;">
  <a href="resources/teaser.png">
  <img src="resources/teaser.png" height="100">
  </a>
</center>
<p></p>

<br><br>
<p style="margin-top:5cm;">
<h2>Abstract</h2>
<div style="font-size:14px"><p align="justify">
    [TODO: Summary of all results and conclusions]
</p></div>
<br><br>

<p style="margin-top:1cm;">
<h2 align='center' style="color: #990000;"><strong>Introduction</strong></h2>
<br>
<div style="font-size:14px"><p align="justify">
     Novel view synthesis is a classical problem at the intersection of computer vision and graphics.
    Given a number of images taken from different camera poses, novel view synthetic aims to produce realistic image of the scene from
    novel angles[1]. Recently, NeRF [2] makes significant progress in this area by utilizing neural scene representation. It takes as input
    a 5D camera pose representation and uses scene-specific multi-layer perceptrons (MLPs) to represent the whole scene. Compared to explict representation like
    point clouds and voxels, this representation is compact and able to reconstruct higher resolution images.
    <br>
        <br>

    Although NeRF performs well in many cases, it still has the following limitation: the learnt scene representation is very rigid and cannot be edited
    flexibly. In other words, all the scene details are fixed in the representation and it is hard to edit one small part without messing up the rest part of the scene.
    To illustrate, suppose we have learnt scene representation of the scene with 100 spheres. If we want to remove a sphere from the scene, we have to
    retrain the network instead of editing the learnt representation directly.
    <br>
    <br>

    To edit the scene representation in a more flexible way, we need to gain more understanding of  the learnt scene representation in different levels.
    Inspired by the recent paper [3] that tries to visualize and understand GAN at the unit-, object-, and scene-level, we present a simple framework to
    study and edit NeRF representation. In specific, we try to answer the following questions: Are there some neurons responsible for controlling
    the emergence of an object in the scene? Can we edit the scene at instance level given a specific operation?
    <br>
    <br>

    In summary, the contributions of our project can be summarized as follows:
    (1) We present a simple framework to study and edit NeRF representation.

    (2) We render a small dataset and conduct some experiments to show that our proposed framework can effectively study and manipulate the NeRF
    representation.


</p></div>

<br><br>
<p style="margin-top:1cm;">
<h2 align='center' style="color: #990000;"><strong>Related Works</strong></h2>

<h3>Novel View Synthesis</h3>
<!--[TODO: recent nerf explosion and short description of some followup works]-->
The task of novel view synthesis is to reconstruct an unseen view given a number of input views. [4] propose a light-field based method which can produce
    realistic results but required a dense set of images. Recent methods [2][5][6] reconstruct novel views with a sparse
    set of input images by using deep neural network. Among these methods, NeRF [2] is the most popular one in the area.
    NeRF adopts an MLP to learn a 5D radiance field of the scene by directly regresses the volume density and RGB colors.
    Since the camera pose is a part of input of the method, the output of NeRF can leverage the view-dependent effect of
    the scene to some extent. However, NeRF still suffers from the following limitations: (1) The inference time is high.
    (2) The learnt representation is scene specific and it is hard to edit it accordingly.
    <br>
    <br>
    To address the first issue, NSVF [7] reduce the inference time of NeRF by defining a set of voxel-bounded implicit
    fields represented by sparse voxel octree to model local features in each cell. FastNeRF [8] presents
    a NeRF-based system capable  of  rendering photorealistic  novel  views thousands of times faster than original NeRF.

    For the second problem, GIRAFFE [9] combine a compositional 3D scene representation into the generative model to allow more controllable image synthesis.
    Representing  scenes  as  compositional  generative  neural  feature  fields  allows  them to  disentangle
    objects from the background so as to manipulate the scene. [10] propose to learn object-centric neural scattering functions (OSFs)
    that can implicitly model object-level light transport using a neural network.
    It enables rendering scenes even when objects or lights move without retraining.
    <br>
    <br>
    The difference between our project and these methods are that we are focusing on study and understand NeRF, and then edit the scene
    according to the insight we learn from the NeRF representation. We believe this can lead to a more general way to manipulate the scene
    based on NeRF.



<!--<h3>Interpretable Networks</h3>-->
<!--[TODO: Ganseeing]-->


<br><br>
<p style="margin-top:1cm;">
<h2 align='center' style="color: #990000;"><strong>Preliminaries</strong></h2>

<h3>Vanilla NeRF:</h3>
[TODO: describe the nerf formution precisely and mathematically]

<h3>Latent Space and Overfitting:</h3>
[TODO: no natural latent space] <br>
[TODO: no generalization across scenes]


<br><br>
<p style="margin-top:1cm;">
<h2 align='center' style="color: #990000;"><strong>Method</strong></h2>
In this section we describe the method we use to find semantically meaningful neurons under different settings.
First, we we describe XXX <br>
Next, we describe YYY <br>
Finally, we show ABC in figs 123 <br>
<h3>Dataset Used: </h3>
We create a basic scene containing 15 spheres using Mitsuba. To find semantically meaningful neurons for scene representation,
we create 15 additional scenes, and each scene is same as the basic scene excepting for the removal of one specific sphere. We

set the near (minimal scene depth for display) and far (maximum scene depth for display) bound to be 0 and 5 respectively. 
In the figure below we show the initial scene containing all the 15 
spheres on the left. In the right, we show one of the additional scene generated where one of the spheres is removed.
<table border="0" cellspacing="0" cellpadding="0" style="margin-top:0cm;">
  <tr>
    <tr>
        <td align="center" valign="middle">
          <a href=".resources/ds_base.gif">
            <img src="resources/ds_base.gif"  align="center" width=350>
          </a>
        </td>
        <td align="center" valign="middle" style="padding-left:100px;">
          <a href="resources/ds_edit_5.gif">
            <img src="resources/ds_edit_5.gif"  align="center" width=350>
          </a>
        </td>
    </tr>
  </tr>
</table>

<h3>Baseline Model: </h3>
We use a PyTorch implementation of NeRF provided <a href="https://github.com/yenchenlin/nerf-pytorch">here</a> for our experiments. 
We do not modify the training hyperparameters and use the default values provided by the authors. 
A configuration file that contains all the hyperparameters used is attached <a href="TBD">here</a>. 

<h3>Modified Training Procedure: </h3>
In addition to the original NeRF method we test the effects of modifying the NeRF training objective. 
Instead of training on a single scene, we apply a transformation to the current image and concatenate a 
one-hot embedding of the transformation to the input of the model. These changes made to the training 
process are shown in the Figure below. 
<table border="0" cellspacing="0" cellpadding="0" style="margin-top:1cm;">
  <tr>
    <tr>
        <td align="center" valign="middle">
          <a href=".resources/architecture.png">
            <img src="resources/architecture.png"  align="center" width=800>
          </a>
        </td>
    </tr>
  </tr>
</table>
<h3>Evaluation</h3>
[TODO: the task is to remove a sphere from the scene] <br>
[TODO: yikes]


<br><br>
<p style="margin-top:1cm;">
<h2 align='center' style="color: #990000;"><strong>Experiments</strong></h2>
<br>
<h3 style="color: #666666;">Search over NeRF Neurons:</h3>
We begin our experiments by investigating the vanilla NeRF model that is trained on 
the full scene (containing all 15 spheres). We iterate through all neurons in the trained model 
one at a time and perturb its output by various scalar factors (1e-2, 1e-1, 10, 100, 100). 
For each neuron and the perturbation applied, we render the scene using a fixed camera pose 
and inspect the change that is induced in the generated scene by the perturbation. 
<br>
Using this procedure above we attempt to find individual neurons that can control meaningful 
attributes of the scene represented. In the figures below we show few interesting neurons that 
correspond the "middle row of spheres", "middle column of spheres", and "darkness of all spheres". 
<br>
<table border="0" cellspacing="0" cellpadding="0" style="margin-top:0.5cm;margin-bottom:0.5cm;">
  <tr>
    <tr>
        <td align="center" valign="middle">
          <a href=".resources/interesting_neurons.png">
            <img src="resources/interesting_neurons.png"  align="center" width=800>
          </a>
        </td>
    </tr>
  </tr>
</table>

Following this observation we attempt to use the same procedure to find individual neuron that 
is capable of editing the scene in a more fine-grained manner. Specifically, we test if this 
method can be used to remove an individual user-specified sphere from the scene. The figure below 
shows the edited scene generated by perturbing the neuron found to be closest to the task. 
We observe that even the best perturbation does not capture the 
desired edit in the scene and introduces numerous artifacts in the other regions of the image. 
<br> <br>
<table border="0" cellspacing="0" cellpadding="0" style="margin-top:0.5cm;margin-bottom:0.5cm;">
  <tr>
    <tr>
        <td align="center" valign="middle">
          <a href=".resources/neuron_18.png">
            <img src="resources/neuron_18.png"  align="center" width=800>
          </a>
        </td>
    </tr>
  </tr>
</table>
<br><br>

<h3 style="color: #666666;">Transformation Conditioned NeRF Training: </h3>
We hypothesize that the training objective of the NeRF does not encourage the 
learning of semantically neuron representations that are disentangled and control the attributes 
in a consistent manner. In order to test this, we modify the NeRF training procedure as shown 
in the Figure YY below. 
<br> <br>
[<strong>FIGURE TODO ::: </strong> modified NeRF training]
<br><br>
The Figure YY below shows that the modified NeRF training is capable of learning a more 
semantic representation of the scene. The modified scene captures the desired edit 
consistently across different camera poses and does not introduce severe artifects in 
other parts of the scene. 

<h3 style="color: #666666;">Quantiative Comparision: </h3>

<h3 style="color: #666666;">Compositionality of Edits: </h3>

<!-- 
<table border="0" cellspacing="0" cellpadding="0">
  <tr>
    <tr>
        <td align="center" valign="middle"><a href=".resources/dcvae_results.png"><img src="resources/dcvae_results.png"  width=800> </a></td>
    </tr>
  </tr>

</table> -->
<!-- <p>&nbsp;</p> -->

<br>
<h2 align='center' style="color: #990000;"><strong>Acknowledgements</strong></h2>
<p align="justify">
  [TODO: ackowledge course staff and other relevant people]
</p>

<br>
<h2>Citations</h2>
<br>
<p><a href="https://cseweb.ucsd.edu/~ravir/6998/papers/p279-chen.pdf">[1]</a> Shenchang Eric Chen and Lance Williams. View interpolationfor image synthesis.SIGGRAPH, 1993. </p>
<p><a href="https://arxiv.org/pdf/2102.13090.pdf">[2]</a> Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng.  Nerf: Representing scenes as neural radiance fields for view synthesis.ECCV, 2020. </p>
<p><a href="https://arxiv.org/pdf/1811.10597.pdf">[3]</a>  David Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua B. Tenenbaum, William T. Freeman, Antonio Torralba.  Nerf: Representing scenes as neural radiance fields for view synthesis.ECCV, 2020. </p>
<p><a href="https://arxiv.org/pdf/1811.10597.pdf">[4]</a>  Marc  Levoy  and  Pat  Hanrahan.   Light  field  rendering.  SIGGRAPH 1996. </p>
<p><a href="https://arxiv.org/pdf/1904.04290.pdf">[5]</a> Moustafa Meshry, Dan B Goldman, Sameh Khamis, HuguesHoppe,  Rohit Pandey,  Noah Snavely, and Ricardo Martin-Brualla. Neural rerendering in the wild. CVPR, 2019. </p>
<p><a href="https://research.fb.com/publications/neural-volumes-learning-dynamic-renderable-volumes-from-images/">[6]</a> Stephen  Lombardi,  Tomas  Simon,  Jason  Saragih,  GabrielSchwartz, Andreas Lehrmann, and Yaser Sheikh. Neural vol-umes:  Learning dynamic renderable volumes from images. ACM Trans. Graph. 2019. </p>
<p><a href="https://arxiv.org/abs/2007.11571">[7]</a>   Lingjie  Liu,  Jiatao  Gu,  Kyaw  Zaw  Lin,  Tat-Seng  Chua,and Christian Theobalt.   Neural sparse voxel fields.arXivpreprint arXiv:2007.11571, 2020. </p>
<!--<p><a href="https://arxiv.org/abs/2010.07492">[8]   Kai  Zhang,  Gernot  Riegler,  Noah  Snavely,  and  VladlenKoltun.  Nerf++:  Analyzing and improving neural radiancefields.   arXiv preprint arXiv:2010.07492, 2020 </a> </p>-->
<p><a href="https://arxiv.org/abs/2103.10380">[8]</a> Stephan J. Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, Julien Valentin.  FastNeRF: High-Fidelity Neural Rendering at 200FPS. CVPR, 2021. </p>
<p><a href="https://arxiv.org/abs/2011.12100">[9]</a> Michael Niemeyer, Andreas Geiger. GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields. CVPR, 2021. </p>
<p><a href="https://arxiv.org/abs/2012.08503">[10]</a> Michelle Guo, Alireza Fathi, Jiajun Wu, Thomas Funkhouser. Object-Centric Neural Scene Rendering. CVPR, 2020. </p>





</div>
</body></html>